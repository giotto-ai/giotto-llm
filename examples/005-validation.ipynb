{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - repeated inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address your questions about modifying vLLM to include special tokens in the output, here's a detailed guide:\n",
    "\n",
    "### 1. How do I modify the vLLM code?\n",
    "\n",
    "**Modify the `_postprocess` method in the `LLM` class to include special tokens during decoding.**\n",
    "\n",
    "In vLLM, the decoding of generated token IDs into text happens in the `_postprocess` method of the `LLM` class. By default, this method uses `skip_special_tokens=True`, which omits special tokens during decoding. To include special tokens, you need to set `skip_special_tokens=False`.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Locate the `_postprocess` method:**\n",
    "\n",
    "   - Open the `llm.py` file in the `vllm` package. The path is typically `vllm/llm.py`.\n",
    "\n",
    "2. **Modify the decoding parameters:**\n",
    "\n",
    "   - Find the line in the `_postprocess` method where the decoding happens. It usually looks like this:\n",
    "\n",
    "     ```python\n",
    "     output_text = self.tokenizer.decode(\n",
    "         output_ids,\n",
    "         skip_special_tokens=True,\n",
    "         clean_up_tokenization_spaces=True)\n",
    "     ```\n",
    "\n",
    "   - Change `skip_special_tokens` to `False`:\n",
    "\n",
    "     ```python\n",
    "     output_text = self.tokenizer.decode(\n",
    "         output_ids,\n",
    "         skip_special_tokens=False,\n",
    "         clean_up_tokenization_spaces=True)\n",
    "     ```\n",
    "\n",
    "3. **Save the changes and restart your application:**\n",
    "\n",
    "   - After modifying the code, ensure you restart any running instances that use vLLM so the changes take effect.\n",
    "\n",
    "**Alternative Approach:**\n",
    "\n",
    "If you prefer not to modify the vLLM source code directly, you can subclass the `LLM` class and override the `_postprocess` method:\n",
    "\n",
    "```python\n",
    "from vllm import LLM\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    def _postprocess(self, outputs):\n",
    "        # Your custom postprocessing code here\n",
    "        for output in outputs:\n",
    "            output_text = self.tokenizer.decode(\n",
    "                output.output_ids,\n",
    "                skip_special_tokens=False,\n",
    "                clean_up_tokenization_spaces=True)\n",
    "            output.output_text = output_text\n",
    "        return outputs\n",
    "\n",
    "# Use your custom LLM class\n",
    "llm = CustomLLM(model=\"facebook/opt-125m\")\n",
    "```\n",
    "\n",
    "### 2. Should I use a vLLM engine?\n",
    "\n",
    "**Yes, using the vLLM engine directly can give you more control over the generation process, including tokenization and decoding.**\n",
    "\n",
    "By interacting with the `Engine` class, you can customize various aspects of the generation pipeline. This approach is especially useful if you need fine-grained control over token handling.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from vllm import Engine, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer with special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-model\", use_fast=False)\n",
    "# Add your special tokens here\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['<SPECIAL_TOKEN>']})\n",
    "\n",
    "engine = Engine(model=\"your-model\")\n",
    "\n",
    "# Prepare prompts and sampling parameters\n",
    "prompts = [\"Your prompt here\"]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Generate tokens\n",
    "request_outputs = engine.generate(prompts, sampling_params)\n",
    "\n",
    "# Decode tokens including special tokens\n",
    "for output in request_outputs:\n",
    "    output_text = tokenizer.decode(\n",
    "        output.output_ids,\n",
    "        skip_special_tokens=False,\n",
    "        clean_up_tokenization_spaces=True)\n",
    "    print(output_text)\n",
    "```\n",
    "\n",
    "### 3. How can I avoid skipping special tokens?\n",
    "\n",
    "**Ensure that `skip_special_tokens` is set to `False` during the decoding step.**\n",
    "\n",
    "Since vLLM doesn't expose the `skip_special_tokens` parameter in its public API, modifying the `_postprocess` method or using the engine directly (as shown above) are effective ways to include special tokens in the output.\n",
    "\n",
    "**Additional Tips:**\n",
    "\n",
    "- **Access Token IDs Directly:**\n",
    "\n",
    "  If you have access to the token IDs, you can decode them manually:\n",
    "\n",
    "  ```python\n",
    "  for output in outputs:\n",
    "      token_ids = output.output_ids  # Ensure this attribute exists\n",
    "      decoded_text = tokenizer.decode(\n",
    "          token_ids,\n",
    "          skip_special_tokens=False,\n",
    "          clean_up_tokenization_spaces=True)\n",
    "      print(decoded_text)\n",
    "  ```\n",
    "\n",
    "- **Update Tokenizer Configuration:**\n",
    "\n",
    "  Make sure your tokenizer is correctly configured with the special tokens added:\n",
    "\n",
    "  ```python\n",
    "  tokenizer.add_special_tokens({'additional_special_tokens': ['<SPECIAL_TOKEN>']})\n",
    "  llm.model.resize_token_embeddings(len(tokenizer))\n",
    "  ```\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- Modify the `_postprocess` method in vLLM to set `skip_special_tokens=False`.\n",
    "- Using the vLLM engine directly can provide more control over decoding.\n",
    "- Always set `skip_special_tokens=False` when decoding to include special tokens in the output.\n",
    "\n",
    "---\n",
    "\n",
    "By following these steps, you should be able to modify vLLM to include special tokens in your generated outputs successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from vllm import AsyncEngine, SamplingParams, GenerationRequest\n",
    "from transformers import AutoTokenizer\n",
    "import asyncio\n",
    "\n",
    "# Initialize tokenizer with special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-model\", use_fast=False)\n",
    "special_tokens_dict = {'additional_special_tokens': ['<SPECIAL_TOKEN_1>', '<SPECIAL_TOKEN_2>']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Initialize the engine\n",
    "engine = AsyncEngine(model=\"your-model\")\n",
    "\n",
    "# Prepare prompts and tokenize them\n",
    "prompts = [\"Your prompt here\"]\n",
    "tokenized_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "\n",
    "# Create sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Create generation requests\n",
    "requests = []\n",
    "for ids in input_ids:\n",
    "    ids_list = ids.tolist()\n",
    "    request = GenerationRequest(\n",
    "        prompt='',\n",
    "        prompt_token_ids=ids_list,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    requests.append(request)\n",
    "\n",
    "# Define async function to run the engine\n",
    "async def generate_outputs(engine, requests):\n",
    "    return await engine.generate(requests)\n",
    "\n",
    "# Run the asynchronous generation\n",
    "outputs = asyncio.run(generate_outputs(engine, requests))\n",
    "\n",
    "# Decode and print outputs\n",
    "for output in outputs:\n",
    "    all_token_ids = output.prompt_token_ids + output.output_token_ids\n",
    "    decoded_text = tokenizer.decode(\n",
    "        all_token_ids,\n",
    "        skip_special_tokens=False,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    print(decoded_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test Llama 3.2 3B finetuned on re-ARC 400x200 for few epochs lr=1e-4\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from llm_prompts.logs import get_named_logger\n",
    "from llm_prompts.reader import ReaderMany\n",
    "from llm_prompts.causal_lm.models import CausalLMWrapper\n",
    "from llm_prompts.wrapper import EvaluationConfig\n",
    "\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# vLLM\n",
    "from vllm import SamplingParams, LLM\n",
    "\n",
    "# New imports for experiments with Lars code\n",
    "from collections import defaultdict\n",
    "from types import SimpleNamespace\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "from transformers.generation import GenerateDecoderOnlyOutput\n",
    "\n",
    "from llm_prompts.type_aliases import Attempts, Grid\n",
    "from llm_prompts.data import Dataset\n",
    "from llm_prompts.utils import RepeatSampler\n",
    "from llm_prompts.prompts.grid_formatter import GridFormatter\n",
    "from llm_prompts.transforms import Transforms, _BackTransformTestOutput, backtransform_test_output\n",
    "from llm_prompts.type_aliases import Grid, OAIMessage, JSONTask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama_3B\"\n",
    "\n",
    "MAX_NUM_TASKS = 1\n",
    "\n",
    "data_config = {\n",
    "    \"dataset_dir\": \"../../kaggle/input\",\n",
    "    \"dataset_type\": \"evaluation\",\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"wrapper\": CausalLMWrapper,\n",
    "    \"wrapper_kwargs\": {\"model_id\": \"models/llama/ID002_best_text_24_10_25_merged_pretrained_llama_1B_short_re_arc_400x200\"},\n",
    "    \"evaluation_config\": {\n",
    "        \"batch_size\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "assert os.path.exists(data_config[\"dataset_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_config[\"wrapper_kwargs\"][\"model_id\"])\n",
    "print(f\"{len(tokenizer)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ReaderMany(\n",
    "    dataset_dir=data_config[\"dataset_dir\"],\n",
    "    dataset_type=data_config[\"dataset_type\"],\n",
    "    read_test_output=True,\n",
    ").read_tasks()\n",
    "\n",
    "tasks = {key: value for i, (key, value) in enumerate(tasks.items()) if key in (\"070dd51e\",)}\n",
    "\n",
    "print(f\">>> {len(tasks)}\")\n",
    "print(f\">>> {tasks.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = model_config[\"wrapper\"](**model_config[\"wrapper_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLATE_TOKENIZER = AutoTokenizer.from_pretrained(model_config[\"wrapper_kwargs\"][\"model_id\"])\n",
    "# collate of CausalLMWrapper\n",
    "def collate_fn_eval(\n",
    "    examples: list[tuple[OAIMessage, int, _BackTransformTestOutput]],\n",
    ") -> dict[str, dict[str, Tensor] | list[int] | list[_BackTransformTestOutput]]:\n",
    "    \"\"\"The collate function.\"\"\"\n",
    "\n",
    "    conversation = [example[0][:2] for example in examples]\n",
    "    encoded_conversation = COLLATE_TOKENIZER.apply_chat_template(\n",
    "        conversation=conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    batch_inputs = COLLATE_TOKENIZER(\n",
    "        encoded_conversation,\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        split_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch_indices = [example[1] for example in examples]\n",
    "    backtransforms = [example[2] for example in examples]\n",
    "    return {\n",
    "        \"batch_inputs\": batch_inputs,\n",
    "        \"batch_indices\": batch_indices,\n",
    "        \"backtransforms\": backtransforms,\n",
    "    }\n",
    "\n",
    "# collate_fn_eval = partial(_collate_fn_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = SimpleNamespace(\n",
    "    batch_size = 1,\n",
    "    num_dataloader_workers = 2,\n",
    "    n_transforms = 2,\n",
    ")\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    n_attempts = 2,\n",
    "    transforms = Transforms(order=\"reorder\", color=\"foreground\", limit_colors=False, rigid=True),\n",
    "\n",
    "    image_resize_factor = None,\n",
    ")\n",
    "\n",
    "print(f\"{data_config.n_transforms=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_formatter = GridFormatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_inputs: dict[str, Tensor],\n",
    "    generation_config: GenerationConfig,\n",
    "):\n",
    "    output = model.generate(\n",
    "        **batch_inputs,\n",
    "        generation_config=generation_config,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def _create_results(\n",
    "    task_attempts: dict[str, list[Grid]],\n",
    "    task_log_likelihoods: dict[str, list[float]],\n",
    "    n_attempts: int | None,\n",
    ") -> dict[str, Attempts]:\n",
    "    \"\"\"Sort attempts by log-likelihood, merge and combine test examples from same tasks\"\"\"\n",
    "    results: dict[str, Attempts] = defaultdict(lambda: defaultdict(list))\n",
    "    for split_task_id in sorted(task_attempts.keys()):\n",
    "        if \"-|-\" in split_task_id:\n",
    "            tokens = split_task_id.split(\"-|-\")\n",
    "            task_id = tokens[0]\n",
    "            test_idx = int(tokens[1])\n",
    "        else:\n",
    "            task_id = split_task_id\n",
    "            test_idx = 0\n",
    "\n",
    "        attempts = task_attempts[split_task_id]\n",
    "        # There can be duplicate attempts, so mean the log likelihood of duplicates\n",
    "        attempt_log_likelihoods: dict[str, list[float]] = defaultdict(list)\n",
    "        for i, attempt in enumerate(attempts):\n",
    "            attempt_log_likelihoods[str(attempt)].append(task_log_likelihoods[split_task_id][i])\n",
    "\n",
    "        grids = [json.loads(attempt) for attempt in attempt_log_likelihoods.keys()]\n",
    "        log_likelihoods = [np.mean(ll) for ll in attempt_log_likelihoods.values()]\n",
    "\n",
    "        idx = np.argsort(log_likelihoods)[::-1]\n",
    "        if n_attempts is not None:\n",
    "            idx = idx[:n_attempts]\n",
    "        results[task_id][test_idx] = [grids[i] for i in idx]\n",
    "    return results\n",
    "\n",
    "def _decode(tokenizer, output_ids: Tensor, input_size: int) -> str:\n",
    "    response: str = tokenizer.batch_decode(\n",
    "        output_ids[:, input_size:],\n",
    "        skip_special_tokens=False,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def _get_log_likelihoods(tokenizer, output: GenerateDecoderOnlyOutput, input_size: int) -> Tensor:\n",
    "    # Remove input tokens, as well as start/end tokens.\n",
    "    generated_tokens = output.sequences[:, input_size:]\n",
    "    # Stack logits to get shape [batch_size, sequence_length, vocab_size]\n",
    "    logits = torch.stack(output.scores, dim=1)\n",
    "    # Compute log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    # Get attention mask (1s for real tokens, 0s for padding)\n",
    "    attention_mask = (generated_tokens != tokenizer.pad_token_id).long()\n",
    "    # Select log probabilities for the generated tokens\n",
    "    log_likelihoods = log_probs.gather(2, generated_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "    # Apply attention mask to ignore padding in log likelihood\n",
    "    masked_log_likelihoods = log_likelihoods * attention_mask\n",
    "    # Compute total log likelihood (sum across all tokens in the sequence)\n",
    "    total_log_likelihood: Tensor = masked_log_likelihoods.sum(dim=-1)\n",
    "\n",
    "    print(f\"{generated_tokens.shape=}\")\n",
    "    print(f\"{logits.shape=}\")\n",
    "    print(f\"{log_probs.shape=}\")\n",
    "    print(f\"{attention_mask.shape=}\")\n",
    "    print(f\"{log_likelihoods.shape=}\")\n",
    "    print(f\"{total_log_likelihood.shape=}\")\n",
    "\n",
    "    return total_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.model.eval()\n",
    "\n",
    "\n",
    "task_grids: dict[str, list[Grid]] = defaultdict(list)\n",
    "task_log_likelihoods: dict[str, list[float]] = defaultdict(list)\n",
    "dataset = Dataset(\n",
    "    tasks=tasks,\n",
    "    prompt_type=\"prompt_solve_short\",\n",
    "    model_type=\"text-to-text\",\n",
    "    transforms=config.transforms,\n",
    "    image_resize_factor=config.image_resize_factor,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=data_config.batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=RepeatSampler(data_config.n_transforms, len(dataset)),\n",
    "    num_workers=data_config.num_dataloader_workers,\n",
    "    collate_fn=collate_fn_eval,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"{len(dataset)=}\")\n",
    "print(f\"{len(dataloader)=}\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=2,\n",
    "    num_beams=2,\n",
    ")\n",
    "\n",
    "print(f\">>> Start loop. {len(dataloader)=}\")\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(f\"Generating batch {i+1} of {len(dataloader)}\")\n",
    "    batch_indices = batch[\"batch_indices\"]\n",
    "    batch_inputs = batch[\"batch_inputs\"]\n",
    "    input_ids = batch_inputs[\"input_ids\"]\n",
    "    backtransforms = batch[\"backtransforms\"]\n",
    "    if generation_config.num_return_sequences > 1:\n",
    "        batch_indices = [\n",
    "            i for i in batch_indices for _ in range(generation_config.num_return_sequences)\n",
    "        ]\n",
    "        backtransforms = [\n",
    "            t for t in backtransforms for _ in range(generation_config.num_return_sequences)\n",
    "        ]\n",
    "    output = _generate(\n",
    "        model=wrapper.model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_inputs=batch_inputs,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    responses = _decode(\n",
    "        tokenizer=tokenizer,\n",
    "        output_ids=output.sequences,\n",
    "        input_size=input_ids.shape[1]\n",
    "    )\n",
    "    log_likelihoods = _get_log_likelihoods(\n",
    "        tokenizer=tokenizer,\n",
    "        output=output,\n",
    "        input_size=input_ids.shape[1]\n",
    "    )\n",
    "    attempts = [\n",
    "        grid_formatter.decode_grid(\n",
    "            str_containing_grid=response,\n",
    "            input_or_output=\"output\",\n",
    "            logger=None,\n",
    "        )\n",
    "        for response in responses\n",
    "    ]\n",
    "    for attempt, log_likelihood, idx, backtransform in zip(\n",
    "        attempts, log_likelihoods, batch_indices, backtransforms, strict=True\n",
    "    ):\n",
    "        if attempt is None:\n",
    "            continue\n",
    "        task_id = dataset.keys[idx]\n",
    "        task_grids[task_id].append(\n",
    "            backtransform_test_output(grid=attempt, backtransform=backtransform)\n",
    "        )\n",
    "        task_log_likelihoods[task_id].append(log_likelihood.item())\n",
    "\n",
    "results: dict[str, Attempts] = _create_results(\n",
    "    task_attempts=task_grids,\n",
    "    task_log_likelihoods=task_log_likelihoods,\n",
    "    n_attempts=100,\n",
    ")\n",
    "\n",
    "total_end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {total_end_time - total_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID_TO_TEST = \"070dd51e\"\n",
    "\n",
    "exp_grid = tasks[TASK_ID_TO_TEST][\"test\"][0][\"output\"]\n",
    "num_attempts = len(results[TASK_ID_TO_TEST][0])\n",
    "first_grid = results[TASK_ID_TO_TEST][0][0]\n",
    "second_grid = results[TASK_ID_TO_TEST][0][0]\n",
    "if num_attempts > 1:\n",
    "    second_grid = results[TASK_ID_TO_TEST][0][1]\n",
    "\n",
    "print(f\"Returned {num_attempts} possible grids to check\")\n",
    "print()\n",
    "\n",
    "print(f\"The first is solving? {first_grid == exp_grid}\")\n",
    "print(f\"The second is solving? {second_grid == exp_grid}\")\n",
    "print(f\"Any is solving? {any(grid == exp_grid for grid in results[TASK_ID_TO_TEST])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(first_grid) == np.array(exp_grid))\n",
    "first_grid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_grid)\n",
    "print(results[TASK_ID_TO_TEST][0][0])\n",
    "print()\n",
    "print(results[TASK_ID_TO_TEST][0][1])\n",
    "print(\"---------\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_named_logger(\n",
    "    name=f\"validation_test\",\n",
    "    log_level=logging.INFO,\n",
    "    enable_log_to_file=True,\n",
    "    project_root=\"../../\",\n",
    "    output_dir=\"logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "results = wrapper.evaluate(\n",
    "        tasks=tasks,\n",
    "        logger=logger,\n",
    "        config=EvaluationConfig(**model_config[\"evaluation_config\"]),\n",
    "    )\n",
    "e = time.time()\n",
    "\n",
    "print(f\">>> {results=}\")\n",
    "print(f\"Time: {e - s:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
