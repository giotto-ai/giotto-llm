{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test vllm inference Llama 3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "- [Extending competition](https://www.kaggle.com/competitions/arc-prize-2024/discussion/536832)\n",
    "\n",
    "## TODOs\n",
    "- Inference with adapters and float16\n",
    "- Inference and pixel metrics\n",
    "- Data augmentations on the fly with formatter\n",
    "- Filter data by number of tokens\n",
    "- Prompts class with config\n",
    "- Config for training in JSON\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# Prompts\n",
    "# from notebooks_utils import create_eval_dataset\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# vLLM\n",
    "from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"finetuned_models/base-llama-32-3B-fp16-4bit\"\n",
    "# LORA_MODEL_ID = \"finetuned_models/tmp_finetuning_llama_3B_max_seq_3072_comb_instr\"\n",
    "# LORA_MODEL_ID = \"tmp_finetuning_llama_3B_max_seq_3072_comb_short\"\n",
    "LORA_MODEL_ID = \"tmp_finetuning_llama_3B_max_seq_3072_comb_descr\"\n",
    "\n",
    "postfix = \"-\".join(LORA_MODEL_ID.split(\"_\")[-3:])\n",
    "MERGED_MODEL_ID = \"finetuned_models/finetuned-llama-32-3B-fp16-4bit-merged\" + \"-\" + postfix\n",
    "assert os.path.exists(MERGED_MODEL_ID), f\"{MERGED_MODEL_ID} does not exist\"\n",
    "\n",
    "PROMPT_FN = None\n",
    "if MERGED_MODEL_ID.endswith(\"instr\"):\n",
    "    PROMPT_FN = prepare_input_v2\n",
    "elif MERGED_MODEL_ID.endswith(\"descr\"):\n",
    "    PROMPT_FN = prepare_input_v3\n",
    "elif MERGED_MODEL_ID.endswith(\"short\"):\n",
    "    PROMPT_FN = prepare_input_short\n",
    "\n",
    "MAX_SEQ_LENGTH = 3072\n",
    "MAX_NUM_EVAL_TASKS = 400\n",
    "\n",
    "print(f\">>> {BASE_MODEL_ID=}\")\n",
    "print(f\">>> {LORA_MODEL_ID=}\")\n",
    "print(f\">>> {MERGED_MODEL_ID=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = create_eval_dataset(\n",
    "    base_model_id=BASE_MODEL_ID,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    max_num_eval_tasks=MAX_NUM_EVAL_TASKS,\n",
    "    prepare_input_fn=PROMPT_FN,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [sample[\"conversations\"][0][\"content\"] for sample in eval_dataset]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_ID)\n",
    "print(tokenizer.pad_token)\n",
    "print(len(tokenizer.encode(prompts[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lora params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "max_loras: int = 1\n",
    "max_lora_rank: int = 16\n",
    "enable_prompt_adapter: bool = False\n",
    "max_prompt_adapters: int = 1\n",
    "max_prompt_adapter_token: int = 0\n",
    "fully_sharded_loras: bool = False\n",
    "lora_extra_vocab_size: int = 256\n",
    "long_lora_scaling_factors: Optional[Tuple[float]] = None\n",
    "lora_dtype: Optional[Union[str, torch.dtype]] = 'auto'\n",
    "max_cpu_loras: Optional[int] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_args = EngineArgs(\n",
    "    model=BASE_MODEL_ID,\n",
    "    quantization=\"bitsandbytes\",\n",
    "    qlora_adapter_name_or_path=LORA_MODEL_ID,\n",
    "    load_format=\"bitsandbytes\",\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=8,\n",
    "    max_loras=1,\n",
    "    fully_sharded_loras=False,\n",
    "    lora_dtype=torch.float16,\n",
    "    tensor_parallel_size=2,\n",
    "    max_seq_len_to_capture=MAX_SEQ_LENGTH,\n",
    "    gpu_memory_utilization=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = LLMEngine.from_engine_args(engine_args)\n",
    "lora_req = LoRARequest(\"lora-test-1\", 1, LORA_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_prompts = create_test_prompts(lora_path)\n",
    "request_id = 0\n",
    "test_prompts = [\n",
    "    (prompt,\n",
    "    SamplingParams(\n",
    "        temperature=0.0,\n",
    "        logprobs=1,\n",
    "        prompt_logprobs=1,\n",
    "        max_tokens=1024\n",
    "    ),\n",
    "    lora_req)\n",
    "    for prompt in prompts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while test_prompts or engine.has_unfinished_requests():\n",
    "    if test_prompts:\n",
    "        prompt, sampling_params, lora_request = test_prompts.pop(0)\n",
    "        print(f\">>> Processing prompt of length {len(prompt)} with {lora_request=} and {sampling_params=}\")\n",
    "        engine.add_request(str(request_id),\n",
    "                            prompt,\n",
    "                            sampling_params,\n",
    "                            lora_request=lora_request)\n",
    "        request_id += 1\n",
    "\n",
    "    request_outputs: List[RequestOutput] = engine.step()\n",
    "    for request_output in request_outputs:\n",
    "        if request_output.finished:\n",
    "            print(\"----------------------------------------------------\")\n",
    "            print(f\"Prompt: {request_output.prompt}\")\n",
    "            print(f\"Output: {request_output.outputs[0].text}\")\n",
    "\n",
    "# Clean up the GPU memory for the next test\n",
    "del engine\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
