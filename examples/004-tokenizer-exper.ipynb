{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add special tokens to tokenizer\n",
    "- Remove tokens from tokenizer\n",
    "- Run LLama model with new tokenizer\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub._login import _login as hf_login\n",
    "hf_login(token=\"\", add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llm_prompts.prompts.grid_formatter import GridFormatter\n",
    "from llm_prompts.prompts.text_prompts import PromptSolveInstrV2\n",
    "from llm_prompts.reader import ReaderPickle\n",
    "from llm_prompts.type_aliases import Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tokenizer = \"../../llm_prompts/tests/llama_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_tokenizer)\n",
    "\n",
    "print(\"Tokenizer name:\", tokenizer.name_or_path)\n",
    "print(\"Tokenizer length:\", len(tokenizer))\n",
    "print(\"Special token map:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_formatter_notebook = GridFormatter()\n",
    "print(grid_formatter_notebook.sE_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_special_tokens = grid_formatter_notebook.get_special_tokens_not_in(tokenizer=tokenizer)\n",
    "print(additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid: Grid = [\n",
    "    [0, 1, 2],\n",
    "    [0, 1, 2],\n",
    "    [4, 4, 4]\n",
    "]\n",
    "\n",
    "grid_formatter_notebook.encode_grid(grid, input_or_output=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add tokens to `tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_special_tokens = {\n",
    "    \"additional_special_tokens\": additional_special_tokens\n",
    "}\n",
    "tokenizer.add_special_tokens(new_special_tokens)\n",
    "\n",
    "path_to_special_tokenizer = path_to_tokenizer + \"_with_special\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(path_to_special_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokenizer = AutoTokenizer.from_pretrained(path_to_special_tokenizer)\n",
    "\n",
    "print(\"Special tokenizer name:\", special_tokenizer.name_or_path)\n",
    "print(\"Special tokenizer length:\", len(special_tokenizer))\n",
    "print(\"Special token map:\", special_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ReaderPickle(\n",
    "    dataset_dir=\"../../synth_data\",\n",
    "    dataset_category=\"re_arc_400x5\",\n",
    "    read_test_output=True,\n",
    ").read_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_task_id = sorted(tasks.keys())\n",
    "task_id = sorted_task_id[0]\n",
    "\n",
    "print(f\"{task_id=}\")\n",
    "print(f\"{sorted_task_id[1]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fn = PromptSolveInstrV2(grid_formatter=grid_formatter_notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "messages = prompt_fn(task=tasks[task_id], idx_i=0)\n",
    "e = time.time()\n",
    "\n",
    "print(f\">>> Time: {1000 * (e -s ):.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenizer) >= 128260\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer.apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer.encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_1B = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "llama_1B.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
