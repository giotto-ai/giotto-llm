{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test inference speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"{np.__version__=}\")\n",
    "print(f\"{torch.__version__=}\")\n",
    "print(f\"{torch.cuda.is_available()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "from typing import Literal\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from giotto_llm.causal_lm.models import CausalLMWrapper\n",
    "from giotto_llm.consts import DEFAULT_ATTEMPT\n",
    "from giotto_llm.data import Dataset\n",
    "from giotto_llm.finetuning.merge import merge_model\n",
    "from giotto_llm.online_fine_tuning.args import parse_arguments_main\n",
    "from giotto_llm.online_fine_tuning.utils import MAP_WRAPPER, OnlineFinetuningConfig\n",
    "from giotto_llm.plot.matplotlib_plots import plot_predictions\n",
    "from giotto_llm.prompts.consts import TYPES_OF_PROMPTS\n",
    "from giotto_llm.prompts.grid_formatter import GridFormatter\n",
    "\n",
    "from giotto_llm.reader import ReaderOneOnlineFinetuning\n",
    "from giotto_llm.transforms import Transforms, transform_task\n",
    "from giotto_llm.utils import is_tf32_supported, write_json\n",
    "# Note: not importing split_tasks_by_test()\n",
    "from giotto_llm.wrapper import EvaluationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"wrapper\": CausalLMWrapper,\n",
    "    \"wrapper_kwargs\": {\n",
    "        \"model_id\": \"\",\n",
    "        \"quantization\": \"no\",\n",
    "    },\n",
    "    \"evaluation_config\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"n_attempts\": 2,\n",
    "        \"n_transforms\": 4,\n",
    "        \"rigid_transforms_all\": False,\n",
    "        \"generation_config\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"num_return_sequences\": 1,\n",
    "            \"num_beams\": 1,\n",
    "        },\n",
    "        \"dfs_sampling\": False,\n",
    "        \"dfs_config\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"threshold\": 0.1,\n",
    "            \"batch_size\": 6,\n",
    "        },\n",
    "        \"selection_with_augmentation\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sft_config(config: OnlineFinetuningConfig) -> SFTConfig:\n",
    "    \"\"\"Get the SFTConfig\"\"\"\n",
    "    sft_config = SFTConfig(\n",
    "        do_eval=not config.kaggle_mode,\n",
    "        output_dir=config.output_dir,\n",
    "        logging_dir=f\"{config.output_dir}/logs\",\n",
    "        eval_strategy=(\n",
    "            \"no\" if config.kaggle_mode else \"epoch\" if config.eval_steps is None else \"steps\"\n",
    "        ),\n",
    "        eval_steps=config.eval_steps,\n",
    "        prediction_loss_only=True,\n",
    "        per_device_train_batch_size=config.per_device_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        max_seq_length=20_000,  # Not used for anything with custom collator\n",
    "        eval_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        torch_empty_cache_steps=1 if config.low_memory is True else None,\n",
    "        fp16_full_eval=config.low_memory,\n",
    "        learning_rate=config.learning_rate,\n",
    "        max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
    "        save_strategy=(\n",
    "            \"no\" if config.kaggle_mode else \"epoch\" if config.eval_steps is None else \"steps\"\n",
    "        ),\n",
    "        save_steps=config.eval_steps,\n",
    "        save_total_limit=config.save_total_limit,  # Can affect memory use\n",
    "        save_only_model=True,\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "        fp16=not is_tf32_supported(),\n",
    "        logging_strategy=\"epoch\" if config.logging_steps is None else \"steps\",\n",
    "        logging_steps=config.logging_steps,\n",
    "        bf16=is_tf32_supported(),\n",
    "        tf32=is_tf32_supported(),\n",
    "        dataloader_num_workers=4,  # Should be sensible default\n",
    "        remove_unused_columns=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        optim=\"adamw_torch_fused\" if config.quantization is None else \"adamw_8bit\",\n",
    "        report_to=None,\n",
    "        gradient_checkpointing=True,  # Set to False for (possibly) faster training at the expense of memory\n",
    "        neftune_noise_alpha=config.neftune_noise_alpha,\n",
    "        use_liger_kernel=True,  # Still runs if not available\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        dataset_text_field=\"\",  # need a dummy field for collator\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
    "        dataloader_pin_memory=not config.low_memory,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    sft_config.remove_unused_columns = False\n",
    "    return sft_config\n",
    "\n",
    "\n",
    "def get_train_dataset(  # type: ignore\n",
    "    config: OnlineFinetuningConfig,\n",
    "    model_type: Literal[\"image-text-to-text\", \"text-to-text\"],\n",
    "    grid_formatter: GridFormatter,\n",
    "    task_name,\n",
    "    demo_tasks,\n",
    ") -> Dataset:\n",
    "    tasks = ReaderOneOnlineFinetuning(\n",
    "        task_name, demo_tasks, test_solutions=None, is_test=False\n",
    "    ).read_tasks()\n",
    "\n",
    "    transforms = Transforms(\n",
    "        test=False,\n",
    "        order=\"reorder\",\n",
    "        color=\"all\" if config.transform_background_color is True else \"foreground\",\n",
    "        limit_colors=model_type == \"image-text-to-text\",\n",
    "        rigid=True,\n",
    "    )\n",
    "    dataset = Dataset(\n",
    "        tasks=tasks,\n",
    "        transforms=transforms,\n",
    "        messages_fn=TYPES_OF_PROMPTS[config.prompt_type](grid_formatter=grid_formatter),\n",
    "        model_type=model_type,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_eval_dataset(  # type: ignore\n",
    "    config: OnlineFinetuningConfig,\n",
    "    model_type: Literal[\"image-text-to-text\", \"text-to-text\"],\n",
    "    grid_formatter: GridFormatter,\n",
    "    task_name,\n",
    "    demo_tasks,\n",
    "    test_solutions,\n",
    ") -> Dataset:\n",
    "    tasks = ReaderOneOnlineFinetuning(\n",
    "        task_name, demo_tasks, test_solutions=test_solutions, is_test=True\n",
    "    ).read_tasks()\n",
    "\n",
    "    transforms = Transforms(\n",
    "        test=False,\n",
    "        order=None,\n",
    "        color=None,\n",
    "        limit_colors=model_type == \"image-text-to-text\",\n",
    "        rigid=False,\n",
    "    )\n",
    "\n",
    "    dataset = Dataset(\n",
    "        tasks=tasks,\n",
    "        transforms=transforms,\n",
    "        messages_fn=TYPES_OF_PROMPTS[config.prompt_type](grid_formatter=grid_formatter),\n",
    "        model_type=model_type,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_eval_results(  # type: ignore\n",
    "    logger,\n",
    "    task_name,\n",
    "    demo_tasks,\n",
    "    test_solutions,\n",
    "    model_config,\n",
    "    submission_save_path,\n",
    "    image_save_path,\n",
    "    wrapper=None,\n",
    "):\n",
    "    logger.info(\"Starting evaluation\")\n",
    "    tasks = ReaderOneOnlineFinetuning(\n",
    "        task_name, demo_tasks, test_solutions=test_solutions, is_test=True\n",
    "    ).read_tasks()\n",
    "    if wrapper is None:\n",
    "        wrapper = model_config[\"wrapper\"](**model_config[\"wrapper_kwargs\"])\n",
    "    else:\n",
    "        logger.info(\"Running Infrence Without Merging Adaptor\")\n",
    "\n",
    "    results = wrapper.evaluate(\n",
    "        tasks=tasks,\n",
    "        logger=logger,\n",
    "        config=EvaluationConfig(\n",
    "            **model_config[\"evaluation_config\"],\n",
    "        ),\n",
    "    )\n",
    "    submission: dict = {task_id: [] for task_id in tasks.keys()}\n",
    "    count_solved = 0\n",
    "    total = 0\n",
    "    for index_task, (task_id, attempts) in enumerate(results.items()):\n",
    "        attempts_task_id = []\n",
    "        for idx_i in range(len(tasks[task_id][\"test\"])):\n",
    "            if idx_i not in attempts:\n",
    "                attempts_task_id.append(\n",
    "                    {\"attempt_1\": DEFAULT_ATTEMPT, \"attempt_2\": DEFAULT_ATTEMPT}\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\">>> Evaluating {idx_i=} for {task_id=}\")\n",
    "                grids = attempts[idx_i]\n",
    "                expected_grid = tasks[task_id][\"test\"][idx_i][\"output\"]\n",
    "\n",
    "                # logger.info(f\">>> Grids\\n{grids}\\n{expected_grid}\")\n",
    "                logger.info(\"---\")\n",
    "\n",
    "                for grid in grids:\n",
    "                    if grid == expected_grid:\n",
    "                        count_solved += 1\n",
    "                        break\n",
    "\n",
    "                attempts_task_id.append(\n",
    "                    {\n",
    "                        \"attempt_1\": grids[0],\n",
    "                        \"attempt_2\": DEFAULT_ATTEMPT if len(grids) == 1 else grids[1],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            total += 1\n",
    "            logger.info(f\">>> Currently {count_solved=}/{total}\")\n",
    "\n",
    "            plot_predictions(\n",
    "                tasks[task_id],\n",
    "                test_id=idx_i,\n",
    "                predictions=attempts_task_id[-1].values(),\n",
    "                save_path=f\"{image_save_path}_{idx_i}.png\",\n",
    "            )\n",
    "\n",
    "        submission[task_id] = attempts_task_id\n",
    "\n",
    "    if count_solved > 0:\n",
    "        logger.info(\"\\033[95m\" + f\"TASK IS SOLVED: {count_solved}/{total}\" + \"\\033[0m\")\n",
    "    else:\n",
    "        logger.info(\"\\033[94m\" + f\"TASK IS NOT SOLVED: {count_solved}/{total}\" + \"\\033[0m\")\n",
    "\n",
    "    with open(submission_save_path, \"w\") as f:\n",
    "        json.dump(submission, f)\n",
    "\n",
    "    logger.info(\"Finished\")\n",
    "\n",
    "    return count_solved, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(logger, task_name, demo_tasks, model_config, submission_save_path, wrapper=None):  # type: ignore\n",
    "    logger.info(f\">>> D: Inside run_inference()\") # ADDED\n",
    "    if wrapper is None:\n",
    "        logger.info(\">>> D: Creating a wrapper with {model_config['wrapper_kwargs']=}\") # ADDED\n",
    "        wrapper = model_config[\"wrapper\"](**model_config[\"wrapper_kwargs\"])\n",
    "\n",
    "    logger.info(f\">>> D: Staring wrapper.evaluate() using DFS or BFS\") # ADDED\n",
    "    results = wrapper.evaluate(\n",
    "        tasks={task_name: demo_tasks},\n",
    "        logger=logger,\n",
    "        config=EvaluationConfig(\n",
    "            **model_config[\"evaluation_config\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    attempts_task_id = []\n",
    "    attempts = results[task_name]\n",
    "    for idx_i in range(len(demo_tasks[\"test\"])):\n",
    "        if idx_i not in attempts:\n",
    "            attempts_task_id.append({\"attempt_1\": DEFAULT_ATTEMPT, \"attempt_2\": DEFAULT_ATTEMPT})\n",
    "            raise ValueError(f\"Didn't attempted for test {idx_i}\")\n",
    "        else:\n",
    "            grids = attempts[idx_i]\n",
    "            attempts_task_id.append(\n",
    "                {\n",
    "                    \"attempt_1\": grids[0],\n",
    "                    \"attempt_2\": DEFAULT_ATTEMPT if len(grids) == 1 else grids[1],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    with open(submission_save_path, \"w\") as f:\n",
    "        json.dump({task_name: attempts_task_id}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "- The default is `'use_unsloth': False,`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# args = parse_arguments_main()\n",
    "args = {\n",
    "    'num_tasks_per_gpu_process': 1,\n",
    "    'kaggle_mode': True,\n",
    "    'use_unsloth': False,\n",
    "    'logger': logging.Logger(name=__name__, level=logging.INFO),\n",
    "    'dataset_dir': './kaggle/input',\n",
    "    'dataset_category': '200_test',\n",
    "    'start_index_tasks': 0,\n",
    "    'end_index_tasks': 1,\n",
    "    'gpu_index': 0,\n",
    "    'model_id': './models/llama/merged_llama_1B_notr_arc_augmented_v5_653be_checkpoint_62046_V001',\n",
    "    'wrapper': 'CausalLM',\n",
    "    'output_dir': 'debug_ttt',\n",
    "    'quantization': '4bit-nf4',\n",
    "    'transform_background_color': True,\n",
    "    'learning_rate': 0.0004,\n",
    "    'batch_size': 1,\n",
    "    'eval_batch_size': 1,\n",
    "    'eval_n_attempts': 2,\n",
    "    'eval_n_transforms': 4,\n",
    "    'eval_rigid_transforms_all': False,\n",
    "    'eval_num_return_sequences': 1,\n",
    "    'eval_num_beams': 1,\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'num_train_epochs': 30,\n",
    "    'neftune_noise_alpha': 10.0,\n",
    "    'prompt_type': 'prompt_solve_short',\n",
    "    'lora_target_modules': None,\n",
    "    'logging_steps': None,\n",
    "    'eval_steps': 1,\n",
    "    'low_memory': False,\n",
    "    'lora_dropout': 0.0,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_r': 8,\n",
    "    'early_stopping_patience': 3,\n",
    "    'save_total_limit': 1,\n",
    "    'untie_word_embeddings': False,\n",
    "}\n",
    "\n",
    "base_config = OnlineFinetuningConfig(\n",
    "    kaggle_mode=args[\"kaggle_mode\"],\n",
    "    use_unsloth=args[\"use_unsloth\"],\n",
    "    model_id=args[\"model_id\"],\n",
    "    wrapper=args[\"wrapper\"],\n",
    "    dataset_dir=args[\"dataset_dir\"],\n",
    "    dataset_category=args[\"dataset_category\"],\n",
    "    output_dir=args[\"output_dir\"],\n",
    "    quantization=args[\"quantization\"],\n",
    "    transform_background_color=args[\"transform_background_color\"],\n",
    "    learning_rate=args[\"learning_rate\"],\n",
    "    per_device_batch_size=args[\"batch_size\"],\n",
    "    eval_batch_size=args[\"eval_batch_size\"],\n",
    "    eval_n_attempts=args[\"eval_n_attempts\"],\n",
    "    eval_n_transforms=args[\"eval_n_transforms\"],\n",
    "    eval_rigid_transforms_all=args[\"eval_rigid_transforms_all\"],\n",
    "    eval_num_return_sequences=args[\"eval_num_return_sequences\"],\n",
    "    eval_num_beams=args[\"eval_num_beams\"],\n",
    "    gradient_accumulation_steps=args[\"gradient_accumulation_steps\"],\n",
    "    num_train_epochs=args[\"num_train_epochs\"],\n",
    "    neftune_noise_alpha=args[\"neftune_noise_alpha\"],\n",
    "    padding_side=None,  # args[\"padding_side\"],\n",
    "    prompt_type=args[\"prompt_type\"],\n",
    "    lora_target_modules=args[\"lora_target_modules\"],\n",
    "    logging_steps=args[\"logging_steps\"],\n",
    "    eval_steps=args[\"eval_steps\"],\n",
    "    low_memory=args[\"low_memory\"],\n",
    "    lora_dropout=args[\"lora_dropout\"],\n",
    "    lora_alpha=args[\"lora_alpha\"],\n",
    "    lora_r=args[\"lora_r\"],\n",
    "    early_stopping_patience=args[\"early_stopping_patience\"],\n",
    "    save_total_limit=args[\"save_total_limit\"],\n",
    "    untie_word_embeddings=args[\"untie_word_embeddings\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index_tasks = args[\"start_index_tasks\"]\n",
    "end_index_tasks = args[\"end_index_tasks\"]\n",
    "gpu_index = args[\"gpu_index\"]\n",
    "logger=args[\"logger\"]\n",
    "\n",
    "print(start_index_tasks, end_index_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up things and reading tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Creating training dataset\")\n",
    "\n",
    "image_prediction_dir = os.path.join(base_config.output_dir, \"images\")\n",
    "raw_prediction_dir = os.path.join(base_config.output_dir, \"predictions\")\n",
    "failed_tasks_dir = os.path.join(base_config.output_dir, \"failed_tasks\")\n",
    "\n",
    "os.makedirs(image_prediction_dir, exist_ok=True)\n",
    "os.makedirs(raw_prediction_dir, exist_ok=True)\n",
    "os.makedirs(failed_tasks_dir, exist_ok=True)\n",
    "\n",
    "tasks_path = os.path.join(\n",
    "    base_config.dataset_dir, f\"arc-agi_{base_config.dataset_category}_challenges.json\"\n",
    ")\n",
    "\n",
    "with open(tasks_path, \"rb\") as f:\n",
    "    tasks_challenges: dict = json.load(f)\n",
    "\n",
    "subset_tasks = sorted(list(tasks_challenges.items()))[start_index_tasks:end_index_tasks]\n",
    "\n",
    "print(len(subset_tasks))\n",
    "print(type(subset_tasks))\n",
    "print(type(subset_tasks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_tasks, total_tasks = 0, 0\n",
    "failed_tasks = {}\n",
    "for i, (task_name, demo_tasks) in enumerate(subset_tasks):\n",
    "    output_dir = os.path.join(base_config.output_dir, f\"{task_name}\")\n",
    "    try:\n",
    "        save_original_model = os.path.join(output_dir, \"original\")\n",
    "        save_merged_model = os.path.join(output_dir, \"merged\")\n",
    "\n",
    "        if os.path.exists(f\"{raw_prediction_dir}/submission_{task_name}.json\"):\n",
    "            logger.info(f\"The task {task_name} is already attempted\")\n",
    "            continue\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(save_original_model, exist_ok=True)\n",
    "        os.makedirs(save_merged_model, exist_ok=True)\n",
    "\n",
    "        config = copy.deepcopy(base_config)\n",
    "        config.output_dir = save_original_model\n",
    "\n",
    "        wrapper_cls = MAP_WRAPPER[config.wrapper]\n",
    "        wrapper = wrapper_cls(  # type: ignore\n",
    "            model_id=config.model_id,\n",
    "            gpu_index=gpu_index,\n",
    "            quantization=config.quantization,\n",
    "            online_finetuning=True,\n",
    "            use_unsloth=config.use_unsloth,\n",
    "        )\n",
    "        logger.info(f\">>> D: {wrapper=}\") # ADDED\n",
    "\n",
    "        lora_config = {\n",
    "            \"target_modules\": (\n",
    "                wrapper._target_modules\n",
    "                if config.lora_target_modules is None\n",
    "                else config.lora_target_modules\n",
    "            ),\n",
    "            \"lora_dropout\": config.lora_dropout,\n",
    "            \"lora_alpha\": config.lora_alpha,\n",
    "            \"r\": config.lora_r,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_rslora\": True,\n",
    "        }\n",
    "\n",
    "        logger.info(f\">>> D: {lora_config=}\") # ADDED\n",
    "        logger.info(f\"Target modules: {lora_config['target_modules']}\")\n",
    "\n",
    "        if config.use_unsloth:\n",
    "            logger.info(\"\\n===========Using Unsloth For Training============\\n\")\n",
    "            lora_config[\"target_modules\"] = [\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ]\n",
    "            lora_config[\"lora_dropout\"] = 0  # unsloth optimized for 0 dropout\n",
    "            wrapper.model = FastLanguageModel.get_peft_model(\n",
    "                wrapper.model,\n",
    "                use_gradient_checkpointing=True,  # \"unsloth\", # True or \"unsloth\" for very long context\n",
    "                random_state=42,\n",
    "                loftq_config=None,  # And LoftQ,\n",
    "                **lora_config,\n",
    "            )\n",
    "        else:\n",
    "            peft_config = LoraConfig(task_type=\"CAUSAL_LM\", **lora_config)\n",
    "\n",
    "        sft_config = get_sft_config(config=config)\n",
    "\n",
    "        print(f\">>> Start training {task_name=}\")\n",
    "\n",
    "        logger.info(\"Initializing model\")\n",
    "\n",
    "        write_json(data=config.dict(), filename=f\"{config.output_dir}/finetuning_config.json\")\n",
    "\n",
    "        callbacks = []  # type: ignore # [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "\n",
    "        train_dataset = get_train_dataset(\n",
    "            config=config,\n",
    "            model_type=wrapper.model_type,\n",
    "            grid_formatter=wrapper.grid_formatter,\n",
    "            task_name=task_name,\n",
    "            demo_tasks=demo_tasks,\n",
    "        )\n",
    "\n",
    "        logger.info(\"Length of train dataset: %d\", len(train_dataset))\n",
    "\n",
    "        eval_dataset = None\n",
    "\n",
    "        try:\n",
    "            trainer = SFTTrainer(\n",
    "                model=wrapper.model,\n",
    "                args=sft_config,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                data_collator=wrapper.collate_fn_train,\n",
    "                peft_config=None if config.use_unsloth else peft_config,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            if \"gradient checkpointing\" in e.args[0]:\n",
    "                sft_config.gradient_checkpointing = False\n",
    "                trainer = SFTTrainer(\n",
    "                    model=wrapper.model,\n",
    "                    args=sft_config,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=eval_dataset,\n",
    "                    data_collator=wrapper.collate_fn_train,\n",
    "                    peft_config=peft_config,\n",
    "                    callbacks=callbacks,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(e)\n",
    "        logger.info(\"Training model\")\n",
    "        trainer.train()\n",
    "        logger.info(\"Saving model\")\n",
    "        if not config.use_unsloth:\n",
    "            trainer.model.to(\"cpu\")\n",
    "            trainer.save_model(config.output_dir)\n",
    "        logger.info(f\"Saving {wrapper.grid_formatter=}\")\n",
    "        wrapper.grid_formatter.save(config.output_dir)\n",
    "\n",
    "        model_config = copy.deepcopy(BASE_CONFIG)\n",
    "\n",
    "        if config.use_unsloth:\n",
    "            model_config[\"wrapper_kwargs\"][\"model_id\"] = config.output_dir  # type: ignore\n",
    "            model_config[\"wrapper_kwargs\"][\"use_unsloth\"] = True  # type: ignore\n",
    "        else:\n",
    "            finetuned_config = OnlineFinetuningConfig.parse_file(\n",
    "                f\"{config.output_dir}/finetuning_config.json\"\n",
    "            )\n",
    "            merge_model(\n",
    "                finetuned_config, adaptor_path=config.output_dir, merge_path=save_merged_model  # type: ignore\n",
    "            )\n",
    "            model_config[\"wrapper_kwargs\"][\"model_id\"] = save_merged_model  # type: ignore\n",
    "            subprocess.run([\"rm\", \"-rf\", config.output_dir])\n",
    "\n",
    "        logger.info(\" -- Evaluating Model --\")\n",
    "\n",
    "        model_config[\"wrapper\"] = wrapper_cls\n",
    "        model_config[\"evaluation_config\"][\"batch_size\"] = config.eval_batch_size  # type: ignore\n",
    "        model_config[\"evaluation_config\"][\"n_attempts\"] = config.eval_n_attempts  # type: ignore\n",
    "        model_config[\"evaluation_config\"][\"n_transforms\"] = config.eval_n_transforms  # type: ignore\n",
    "        model_config[\"evaluation_config\"][\"rigid_transforms_all\"] = config.eval_rigid_transforms_all  # type: ignore\n",
    "        model_config[\"evaluation_config\"][\"generation_config\"][  # type: ignore\n",
    "            \"num_return_sequences\"\n",
    "        ] = config.eval_num_return_sequences\n",
    "        model_config[\"evaluation_config\"][\"generation_config\"][  # type: ignore\n",
    "            \"num_beams\"\n",
    "        ] = config.eval_num_beams\n",
    "\n",
    "        logger.info(f\"Config: {model_config}\")\n",
    "\n",
    "        # if config.kaggle_mode:\n",
    "        run_inference(  # type: ignore\n",
    "            logger,\n",
    "            task_name,\n",
    "            demo_tasks,\n",
    "            model_config,\n",
    "            f\"{raw_prediction_dir}/submission_{task_name}.json\",\n",
    "        )\n",
    "        logger.info(f\">>> D: written predictions to {raw_prediction_dir}/submission_{task_name}.json\") # ADDED\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\033[91m\" + f\"Error in task {task_name}\" + \"\\033[0m\")\n",
    "        print(\"\\033[91m\" + str(e) + \"\\033[0m\")\n",
    "        # remove the dir of the task\n",
    "        failed_tasks[task_name] = str(e)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # remove the original and merged models\n",
    "    subprocess.run([\"rm\", \"-rf\", output_dir])\n",
    "\n",
    "with open(f\"{failed_tasks_dir}/failed_tasks_gpu_{gpu_index}.json\", \"w\") as f:\n",
    "    json.dump(failed_tasks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
